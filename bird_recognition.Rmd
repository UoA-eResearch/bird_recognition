---
title: "Bird Recognition"
output:
  html_document:
    df_print: paged
    toc: yes
  html_notebook:
    toc: yes
    toc_float: yes
---

GitHub repo: https://github.com/UoA-eResearch/bird_recognition

## Load the necessary packages

```{r setup, warning=FALSE}
rm(list=ls(all=TRUE))
knitr::opts_chunk$set(echo=TRUE, message=FALSE, warning = FALSE)
knitr::opts_chunk$set(fig.width = 12, fig.height = 8, fig.align = 'center')

library(bioacoustics)
library(tuneR)
library(seewave)
library(dplyr)
library(tools)
library(ranger)
library(randomForest)
library(stringr)
library(keras)
library(ggplot2)
```

## Load the wave files. Separate out the metadata from the filename into columns. 

```{r load}
files = list.files("wav_files_playback", "*.wav", full.names=TRUE)
files_without_extension = basename(file_path_sans_ext(files))
wavs = setNames(
  lapply(
    files,
    read_audio
  ),
  files_without_extension
)
metadata = data.frame(str_split_fixed(files_without_extension, "_", 3))
metadata = cbind(metadata, files_without_extension)
colnames(metadata) = c("birdid", "calltype", "idnumber", "filename")
```

Render some spectrograms
```{r visualise spectro, eval=FALSE, include=FALSE}
oscillo(wavs[[1]])
birds = unique(metadata$birdid)
filtered = filter(metadata, birdid == birds[1])
for (i in 1:nrow(filtered)) {
  audio = wavs[[filtered$filename[i]]]
  spectro(audio, main=filtered$filename[i])
}
```

## Split dataset into train / test. The test dataset will contain one row for each combination of birdid / calltype

```{r split, echo=FALSE}
#set.seed(1337)
test_row <- sample(1:3,1)
test_row
test <- metadata %>% 
          group_by(birdid, calltype) %>% 
          filter(row_number()==test_row) %>% 
          glimpse()
          
train <-  metadata %>% 
          group_by(birdid, calltype) %>% 
          filter(row_number()!=test_row) %>% 
          glimpse()
```

## Visualise the train/test distribution
```{r}
metadata %>%
  group_by(birdid,calltype) %>% 
  mutate(isTrain=row_number()!=test_row) %>% 
  ungroup() %>% 
  ggplot(aes(birdid,fill=isTrain)) +
    geom_bar(stat = "count") +
    facet_grid(calltype~.) +
    theme_minimal()
```

```{r rf}
mels = sapply(wavs, melfcc, numcep = 20) # Calculate all MFCCs
maxMels <- do.call(max,mels) # Calculate the max across the whole list of matrices to scale the predictor values between 0-1 later

trainMels = mels[train$filename] # Select the MFCCs corresponding to the training dataset
# trainM = do.call(rbind, trainMels) # melfcc gives a matrix - rbind to cast from 3D to 2D across the whole training dataset
# trainM.labels = rep(train$birdid, lapply(trainMels, function(x) dim(x)[1])) # Create birdid labels for each MFCC
# rownames(trainM) = trainM.labels

testMels = mels[test$filename]
# testM = do.call(rbind, testMels)
# testM.labels = rep(test$birdid, lapply(testMels, function(x) dim(x)[1]))
# rownames(testM) = testM.labels

```

Audio recordings have different lengths
```{r audio_len, echo=FALSE}
audio_len_train = sapply(trainMels, dim)
audio_len_test = sapply(testMels,dim)
boxplot(audio_len_train[1,])
hist(audio_len_train[1,], breaks = 50)
hist(audio_len_test[1,], breaks = 50)
```

```{r}
### Adding more resolution to the spectrogram
# # Define the function
# addResolution <- function(matrixI) {
#   test <- matrix(0,141,20)
#   dimension <- dim(matrixI)
#   test[1:dimension[1], 1:dimension[2]] <- matrixI
#   test
# }

timestepTake <- 20
# Only extract 20 first timesteps
extract20 <- function(matrixI) {
  test <- matrix(0,timestepTake,20)
  dimensionM <- dim(matrixI)
  timestepM <- min(timestepTake,dimensionM[1])
  test[1:timestepM, 1:dimensionM[2]] <- matrixI[1:timestepM, 1:dimensionM[2]]
  test
}

# # Adding more elements to the matrices
# trainMels_stretched <- lapply(trainMels, addResolution)
# testMels_stretched <- lapply(testMels, addResolution)

# Only taking the first 20 timesteps othwerwise zero
trainMels_20 <- lapply(trainMels, extract20)
testMels_20 <- lapply(testMels, extract20)
```

```{r echo=FALSE}
# # Draw spectrograms
# dim(trainMels_stretched[[1]]/maxMels)
# image(trainMels_stretched[[1]]/maxMels)
# 
# dim(testMels_stretched[[10]])
# image(testMels_stretched[[10]])

# Draw spectrograms
dim(trainMels_20[[1]]/maxMels)
image(trainMels_20[[1]]/maxMels)

dim(testMels_20[[10]])
image(testMels_20[[10]])
```

```{r}
# # Flatten the matrices out
# trainMels_stretched <- matrix(unlist(trainMels_stretched), ncol=(141*20), byrow=TRUE)
# testMels_stretched <- matrix(unlist(testMels_stretched), ncol=(141*20), byrow=TRUE)
# 
# # Normalise
# trainMels_stretched <- trainMels_stretched/maxMels
# testMels_stretched <- testMels_stretched/maxMels

# Flatten the matrices out
trainMels_20 <- matrix(unlist(trainMels_20), ncol=(20*20), byrow=TRUE)
testMels_20 <- matrix(unlist(testMels_20), ncol=(20*20), byrow=TRUE)

# Normalise
trainMels_20 <- trainMels_20/maxMels
testMels_20 <- testMels_20/maxMels
```


## Using Random-Forest
```{r}
rf = randomForest(trainMels_20, train$birdid, xtest=testMels_20, ytest=test$birdid, importance = FALSE, proximity = FALSE, replace = TRUE, ntree = 1000, mtry = 20)
rf

round(sum(diag(rf$test$confusion)) / sum(rf$test$confusion[,1:(ncol(rf$test$confusion) -1)]) * 100, 2)
```

Visualise wrong predictions

```{r}
fn <- rf$test$predicted != test$birdid

for (i in 1:length(test$birdid)) {
  if (fn[i]) { # If it is false negative
    par(mfrow=c(4,4))
    image(matrix(unlist(testMels_20[i,]),ncol = 20, byrow = TRUE))
    title(paste0(rf$test$predicted[i],">",test$birdid[i]))
    # Show ground truth bird's spectrograms in train dataset
    groundTBird <- train %>% 
      filter(birdid == test$birdid[i])
    groundTBird <- mels[groundTBird$filename]
    for (gt in groundTBird) {
      image(gt)
      title(test$birdid[i])
    }
    # # Show false predicted bird's spectrograms in train dataset
    # predBird <- test %>% 
    #   filter(birdid == rf$test$predicted[i])
    # predBird <- mels[predBird$filename]
    # for (gt in predBird) {
    #   image(gt)
    #   title(rf$test$predicted[i])
    #}
  }
}

```


What about adding column means for each timestep?
```{r}
# Feature engineering
## Add column means or timestep means
colnames(trainMels_20) <- c(paste0("pix",1:400))
colnames(testMels_20) <- c(paste0("pix",1:400))

addColMeans <- function(matrixI) {
  mcol <- matrix(nrow=nrow(matrixI), ncol=timestepTake)
  for (i in 1:timestepTake) {
    mcol[,i] <- rowMeans(matrixI[,i+20*(0:19)])
  }
  colnames(mcol) <- c(paste0("m",1:20))
  results <- cbind(matrixI, mcol)
}

trainMels_20_M <- addColMeans(trainMels_20)
testMels_20_M <- addColMeans(testMels_20)
trainMels_20_M <- cbind(trainMels_20_M,train$birdid)
testMels_20_M <- cbind(testMels_20_M,test$birdid)

colnames(trainMels_20_M)[ncol(trainMels_20_M)] <- "birdid"
colnames(testMels_20_M)[ncol(testMels_20_M)] <- "birdid"
```

# Train new Random Forest model
```{r}
M_ranger <- ranger(dependent.variable.name = "birdid", data = trainMels_20_M, mtry = 20, classification = TRUE, num.trees = 2000, importance = "impurity")
plot(M_ranger$variable.importance, pch=19, col="#00000030")

t_M <- table(predict(M_ranger,testMels_20_M)$predictions, test$birdid)
round(sum(diag(t_M)) / sum(t_M) * 100, 2)
```
Adding timesteps means doesn't boost performance

## Deep learning classification with the R interface to Keras based on MFCC

```{r keras, warning=FALSE}
# Assigning input and output variables
X_train <-  trainMels_20
X_test <-  testMels_20
Y_train <-  to_categorical(as.integer(train$birdid) -1)
Y_test <-  to_categorical(as.integer(test$birdid) - 1)

# Build the sequential model
model = keras_model_sequential()
model %>%
  # Input shape layer = c(samples, rows, cols, channels)
  layer_reshape(input_shape=ncol(X_train),target_shape=c(20,20,1)) %>% 
  layer_conv_2d(64, c(3,3), c(1,1), padding='same') %>%
  layer_activation("relu") %>%
  layer_conv_2d(64, c(3,3), c(1,1), padding='same') %>%
  layer_activation("relu") %>%
  layer_max_pooling_2d(pool_size = c(2,2)) %>%
  layer_dropout(0.5) %>% 
  layer_conv_2d(128, c(3,3), c(1,1), padding='same') %>%
  layer_activation("relu") %>%
  #layer_max_pooling_2d(pool_size = c(2,2)) %>% 
  layer_conv_2d(128, c(3,3), c(1,1), padding='same') %>%
  layer_activation("relu") %>%
  layer_average_pooling_2d(pool_size = c(2,2)) %>% 
  layer_dropout(0.5) %>% 
  # layer_conv_2d(256, c(3,3), c(1,1), padding='same') %>%
  # layer_activation("relu") %>%
  # layer_conv_2d(256, c(3,3), c(1,1), padding='same') %>%
  # layer_global_average_pooling_2d() %>% # Average pooling layer
  # layer_dropout(0.5) %>%
  layer_flatten() %>%
  layer_dense(64, activation ='relu') %>% 
  layer_dropout(0.5) %>% 
  layer_dense(64, activation ='relu') %>% 
  layer_dropout(0.5) %>%
  #layer_average_pooling_2d(pool_size = c(1,1)) %>% 
  layer_dense(units = ncol(Y_train),  activation='softmax')
#layer_dropout(0.5) %>%

# Model compile
model %>% compile(loss = 'categorical_crossentropy',
                 optimizer = "adam",
                 metrics = "categorical_accuracy")


# Add a callback to reduce the learning rate when reaching the plateau
reduce_lr <- callback_reduce_lr_on_plateau(monitor = 'loss', factor = 0.5,
                                           patience = 50, min_lr = 0.0001)
# Start learning
history = model %>% fit(X_train, Y_train, epochs = 200,
             validation_data = list(X_test, Y_test),
             verbose = 1, callbacks = reduce_lr, verbose=0)
```

```{r keras-plot}
plot(history)
```

```{r keras-confusion-matrix}
birdPred <- model %>% predict_classes(X_test)
t_CNN <- table(birdPred,test$birdid)
round(sum(diag(t_CNN)) / sum(t_CNN) * 100, 2)
```

randomForest did better.
